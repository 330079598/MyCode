<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>SparkCore基础（一）</title>
    <style type="text/css" media="all">
      body {
        margin: 0;
        font-family: "Helvetica Neue", Helvetica, Arial, "Hiragino Sans GB", sans-serif;
        font-size: 14px;
        line-height: 20px;
        color: #777;
        background-color: white;
      }
      .container {
        width: 700px;
        margin-right: auto;
        margin-left: auto;
      }

      .post {
        font-family: Georgia, "Times New Roman", Times, "SimSun", serif;
        position: relative;
        padding: 70px;
        bottom: 0;
        overflow-y: auto;
        font-size: 16px;
        font-weight: normal;
        line-height: 25px;
        color: #515151;
      }

      .post h1{
        font-size: 50px;
        font-weight: 500;
        line-height: 60px;
        margin-bottom: 40px;
        color: inherit;
      }

      .post p {
        margin: 0 0 35px 0;
      }

      .post img {
        border: 1px solid #D9D9D9;
      }

      .post a {
        color: #28A1C5;
      }
    </style>
  </head>
  <body>
    <div class="container">
      <div class="post">
        <h1 class="title">SparkCore基础（一）</h1>
        <div class="show-content">
          <h1>* SparkCore基础（一）</h1><p>学习Spark，首先要熟悉Scala，当然你说你会Python或者Java能不能玩Spark？能！但是不推荐，首推Scala，因为Scala非常便捷，而且Scala有非常好的交互式编程体验（当然了，在这里，Python也不差）。其次呢，我们要对Hadoop的MapReduce要是有一定的了解。不然，学习起来，是会稍微费点功夫。好，不扯这么多了，相关的故事啊，疑问啊可以评论留言询问或者百度查询，我们现在直接进入正题。</p><div class="image-package">
<img src="http://upload-images.jianshu.io/upload_images/4951489-1481a37cafb09c43.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" data-original-src="http://upload-images.jianshu.io/upload_images/4951489-1481a37cafb09c43.png?imageMogr2/auto-orient/strip" data-image-slug="1481a37cafb09c43" data-width="495" data-height="123"><br><div class="image-caption"></div>
</div><h2>Spark特征简述</h2><p><b>* Spark是什么</b></p><p><b>官方描述：Spark is a fast and general engine for large-scale data processing</b></p><p>** Spark是一个快速的，通用的，大数据规模的运算引擎。这是一个非常精准的描述。</p><p>** Spark是基于MapReducer实现的通用的分布式计算框架，所以它继承了MapReduce的优点，同时还支持将Job运算任务产生的中间结果和最终结果保存在内存中。</p><p><b>* Spark优势</b></p><p>** Spark的中间数据放到内存中，对于迭代运算效率更高</p><p>** 运算速度奇快</p><p>** 更灵活的数据操作，比如：map, filter, flatMap, sample, groupByKey, reduceByKey, union, join, cogroup, mapValues, sort,partionBy等等</p><p><b>* Spark不适合做什么</b></p><p>** 不适合做增量变化的应用模型</p><p><b>* Spark支持语言</b></p><p>Java、Scala、Python</p><p><b>* 适用场景讨论</b></p><p>** 适用于需要多次操作特定数据集的应用场合。需要反复操作的次数越多，所需读取的数据量越大，受益越大，数据量小但是计算密集度较大的场*合，受益就相对较小。<br></p><h2>Spark下载</h2><p>一般情况下，我们使用spark之前，都需要下载源码，然后根据自己的集群环境（也就是Hadoop版本）进行编译，然后再安装使用。</p><p><b>Spark下载：</b></p><p>http://spark.apache.org/downloads.html<br></p><p>打开页面后，做出如下选择，即可开始下载源码</p><div class="image-package">
<img src="http://upload-images.jianshu.io/upload_images/4951489-12dbbe07bf3cf69a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" data-original-src="http://upload-images.jianshu.io/upload_images/4951489-12dbbe07bf3cf69a.png?imageMogr2/auto-orient/strip" data-image-slug="12dbbe07bf3cf69a" data-width="850" data-height="279"><br><div class="image-caption">在这里我们使用1.6.1的源码</div>
</div><h2>Spark编译</h2><p>在此我们简单介绍两种方式：</p><p><b>** SBT编译</b></p><p>这是一个类似Maven的仓库，基于Scala</p><p><b>** Maven编译</b></p><p>命令：</p><p><br></p><div class="image-package">
<img src="http://upload-images.jianshu.io/upload_images/4951489-aa0c3db4960edd01.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" data-original-src="http://upload-images.jianshu.io/upload_images/4951489-aa0c3db4960edd01.png?imageMogr2/auto-orient/strip" data-image-slug="aa0c3db4960edd01" data-width="452" data-height="215"><br><div class="image-caption"></div>
</div><p><b>** make-distribution.sh编译</b></p><p>修改源码根目录下的<b>make-distribution.sh</b>文件，修改内容如图：<br></p><p><b></b></p><div class="image-package">
<img src="http://upload-images.jianshu.io/upload_images/4951489-06b6e00db61de98c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"><br><div class="image-caption"></div>
</div><p>依次为：配置Spark版本，Scala版本，Hadoop版本，是否支持Hive，1为支持</p><p>配置镜像：注意，如果编译的是原版，请添加此镜像，如果编译的是CDH版本的，请注意去掉此镜像。</p><div class="image-package">
<img src="http://upload-images.jianshu.io/upload_images/4951489-9857b0f2c04e1e6e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" data-original-src="http://upload-images.jianshu.io/upload_images/4951489-9857b0f2c04e1e6e.png?imageMogr2/auto-orient/strip" data-image-slug="9857b0f2c04e1e6e" data-width="695" data-height="150"><br><div class="image-caption"></div>
</div><p><br></p><p>配置域名解析服务器：</p><p>$ sudo vi /etc/resolv.conf，配置如下：</p><p>nameserver 8.8.8.8</p><p>nameserver 8.8.4.4</p><p>最后执行命令：</p><div class="image-package">
<img src="http://upload-images.jianshu.io/upload_images/4951489-ae5d2d237a784b35.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" data-original-src="http://upload-images.jianshu.io/upload_images/4951489-ae5d2d237a784b35.png?imageMogr2/auto-orient/strip" data-image-slug="ae5d2d237a784b35" data-width="476" data-height="147"><br><div class="image-caption">注意要支持yarn和hive</div>
</div><p><b>世界充满爱之编译好的Spark传送门（分别包含包含Apache和CDH版本的）：</b></p><p>链接：http://pan.baidu.com/s/1eRBJtjs 密码：t03u<br></p><h2>Spark运行模式</h2><p><b>** Local</b></p><p>即本地模式</p><p><b>** Standalone</b></p><p>即Spark自带的集群模式，分为Master节点和Worker节点，顾名思义，一个管理者，多个干活的。：）</p><p><b>** Yarn</b></p><p>国内相当主流的一种运行部署模式，只是目前Yarn分配的Container是不能够动态伸缩的，后续可能会考虑支持。</p><p><b>** Mesos</b></p><p>Spark在出生的时候就考虑支持该框架，很灵活，但国内使用似乎不多，感兴趣请自行研究之。</p><h2>Spark安装部署</h2><p>将Spark解压出来，然后到conf目录下，自己将template文件拷贝出文后提到的文件进行配置即可，在之前的章节我们已经提到过很多次，此步骤想必应该非常熟练了，不再赘述了。</p><p><b>Local模式：</b></p><p><b>spark-env.sh 文件配置如下：<br></b></p><div class="image-package">
<img src="http://upload-images.jianshu.io/upload_images/4951489-ebaae121e6ccf7f5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" data-original-src="http://upload-images.jianshu.io/upload_images/4951489-ebaae121e6ccf7f5.png?imageMogr2/auto-orient/strip" data-image-slug="ebaae121e6ccf7f5" data-width="1007" data-height="358"><br><div class="image-caption"></div>
</div><h2>Spark测试案例之Local模式</h2><p>在案例开始前，请确保你的HDFS是可用的，并且spark-shell在active的NameNode节点上运行。此刻建议你已经熟知Hadoop中MapReduce的编写过程以及运行原理。</p><h4>案例一：基于本地模式的WordCount，words.txt中的内容：</h4><div class="image-package">
<img src="http://upload-images.jianshu.io/upload_images/4951489-534b29780e1d5393.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" data-original-src="http://upload-images.jianshu.io/upload_images/4951489-534b29780e1d5393.png?imageMogr2/auto-orient/strip" data-image-slug="534b29780e1d5393" data-width="350" data-height="64"><br><div class="image-caption"></div>
</div><p><b>Step1、进入spark根目录使用$ bin/spark-shell命令启动spark，如下图：</b></p><div class="image-package">
<img src="http://upload-images.jianshu.io/upload_images/4951489-f6fa9b7c1d3718a6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" data-original-src="http://upload-images.jianshu.io/upload_images/4951489-f6fa9b7c1d3718a6.png?imageMogr2/auto-orient/strip" data-image-slug="f6fa9b7c1d3718a6" data-width="1727" data-height="473"><br><div class="image-caption"></div>
</div><p><b><b>Step2、读取/input/words.txt文件，尝试检查一下words.txt文件有多少行数据，操作如下：</b><br></b></p><p>scala&gt; val rdd = sc.textFile("/input/words.txt")</p><div class="image-package">
<img src="http://upload-images.jianshu.io/upload_images/4951489-fa7581e43873bfee.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" data-original-src="http://upload-images.jianshu.io/upload_images/4951489-fa7581e43873bfee.png?imageMogr2/auto-orient/strip" data-image-slug="fa7581e43873bfee" data-width="1295" data-height="173"><br><div class="image-caption"></div>
</div><p>scala&gt; rdd.count<br></p><div class="image-package">
<img src="http://upload-images.jianshu.io/upload_images/4951489-8636eb479ab3e627.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" data-original-src="http://upload-images.jianshu.io/upload_images/4951489-8636eb479ab3e627.png?imageMogr2/auto-orient/strip" data-image-slug="8636eb479ab3e627" data-width="1614" data-height="642"><br><div class="image-caption">当然了，统计词频，这个步骤可以省略，在此只是想验证一下自己读取到的数据有没有问题</div>
</div><p>好，大家可以看到，有3行数据，每一行都有若干英文单词。那么这里面涉及到几个问题需要拿出来讨论一下：</p><p><b>1、什么是rdd？</b></p><p>RDD is a fault-tolerant collection of elements that can be operated on in parallel，RDD是弹性分布式数据集，全称Resilient Distributed Datasets，具有分布式，高容错性等特点，在这里，刚开始接触的话，你可暂且理解为一个集合就可以了，一个数据集合。<br></p><p><b>2、什么是sc？</b></p><p>sc的全称是SparkContext，即Spark的上下文对象，这个理解可以类比于在Hadoop阶段我们在MapReduce中接触到的Context，不管是读取文件还是其他数据操作，都依赖于SparkContext的实例化。在这里，sc即一个实例化好的SparkContext对象。</p><p>我们通过sc.textFile方法读取到HDFS系统中存放的words.txt文件信息，该方法返回一个RDD对象，之后我们通过rdd对象调用count方法，来查看读取到的文件中数据有多少行。</p><p><b><b><b>Step3、利用得到的rdd对象进行数据的拆分，即，每一个单词都拆分成一个RDD对象，比如类似这样的理解：RDD&lt;String&gt; rdd = new RDD("hadoop")；那么使用scala在spark中如何做呢？请看：</b><br></b></b></p><p>scala&gt; val wordRdd = rdd.flatMap(line =&gt; line.split(" "))<br></p><div class="image-package">
<img src="http://upload-images.jianshu.io/upload_images/4951489-c80abf66fcb009bf.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" data-original-src="http://upload-images.jianshu.io/upload_images/4951489-c80abf66fcb009bf.png?imageMogr2/auto-orient/strip" data-image-slug="c80abf66fcb009bf" data-width="966" data-height="60"><br><div class="image-caption"></div>
</div><p>然后我们使用wordRdd显示一下第一个单词看一看：</p><p>scala&gt; wordRdd.first<br></p><div class="image-package">
<img src="http://upload-images.jianshu.io/upload_images/4951489-83e4707e49ed0094.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" data-original-src="http://upload-images.jianshu.io/upload_images/4951489-83e4707e49ed0094.png?imageMogr2/auto-orient/strip" data-image-slug="83e4707e49ed0094" data-width="1445" data-height="509"><br><div class="image-caption"></div>
</div><p><b><b><b><b>Step4、将分割出来的每一个单词做Map映射</b><br></b></b></b></p><p>scala&gt; val mapRdd = wordRdd.map(word =&gt; (word, 1))<br></p><div class="image-package">
<img src="http://upload-images.jianshu.io/upload_images/4951489-b3fd25459a9e719f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" data-original-src="http://upload-images.jianshu.io/upload_images/4951489-b3fd25459a9e719f.png?imageMogr2/auto-orient/strip" data-image-slug="b3fd25459a9e719f" data-width="979" data-height="53"><br><div class="image-caption"></div>
</div><p>这是scala的高阶函数，注意不理解请重新复习Scala语言。该语句的意思是：将wordRdd中存放的单词映射为一个tuple元组，元组中有两个元素，第一个元素为单词，第二个元素为当前单词本次的个数，固定为1，这个1就像Hadoop阶段中Map的LongWritable一样，这个word就像Text一样。</p><p><b>Step5、这一步要做的就是讲map映射出来的数据集进行reduce运算</b></p><p>scala&gt; val reduceRdd = mapRdd.reduceByKey((x, y) =&gt; x + y)<br></p><div class="image-package">
<img src="http://upload-images.jianshu.io/upload_images/4951489-f6f4d09ddfb42f8e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" data-original-src="http://upload-images.jianshu.io/upload_images/4951489-f6f4d09ddfb42f8e.png?imageMogr2/auto-orient/strip" data-image-slug="f6f4d09ddfb42f8e" data-width="1019" data-height="58"><br><div class="image-caption"></div>
</div><p>该行代码的意思是将某一个单词的好多个1（当然如果进行Combine操作了，也许可能不是多个1，如果你无法理解我这一句在说什么，请继续前进，然后重新复习Hadoop的MapReduce相关知识点）进行相加运算。</p><p><b>Step6、查看一下结果</b></p><p>scala&gt; reduceRdd.collect<br></p><div class="image-package">
<img src="http://upload-images.jianshu.io/upload_images/4951489-02e4044d646ec1b8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" data-original-src="http://upload-images.jianshu.io/upload_images/4951489-02e4044d646ec1b8.png?imageMogr2/auto-orient/strip" data-image-slug="02e4044d646ec1b8" data-width="1388" data-height="79"><br><div class="image-caption"></div>
</div><p>显示出来了，而且执行过程非常的迅速，你懂得。</p><p>当然了，以上的操作，完全可以使用一句话来实现，并且代码的体现形式可以非常骚气，如：</p><p><b>scala&gt; sc.textFile("/input/words.txt").flatMap(_.split(" ")).map((_, 1)).reduceByKey(_+_).collect</b><br></p><div class="image-package">
<img src="http://upload-images.jianshu.io/upload_images/4951489-4f5e370091e64f00.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" data-original-src="http://upload-images.jianshu.io/upload_images/4951489-4f5e370091e64f00.png?imageMogr2/auto-orient/strip" data-image-slug="4f5e370091e64f00" data-width="1897" data-height="974"><br><div class="image-caption"></div>
</div><p><b>Step7、当然了结果也可以输出到HDFS系统当中，比如：</b><br></p><p>scala&gt; reduceRdd.saveAsTextFile("/output/spark/test01")</p><h4>案例二：基于案例一，进行二次排序，即，将统计出的词频结果按照降序或者升序排列</h4><p>sc.textFile("/input/words.txt")</p><p>.flatMap(_.split(" "))</p><p>.map((_, 1))</p><p>.reduceByKey(_ + _)</p><p>.map(x =&gt; (x._2, x._1))</p><p>.sortByKey()</p><p>.map(x =&gt; (x._2, x._1))</p><p>.collect</p><p><b><b>Step1、得到案例一的统计好的词频结果，然后做一个map映射，将单词和单词出现的次数颠倒过来，也就是说，(hadoop, 1)变成(1, hadoop)，这样做的原因是因为OrderedRDDFunctions类中有一个方法叫做：sortByKey，意思是按照Key的大小进行排序，默认参数是升序，如图：</b></b><br></p><div class="image-package">
<img src="http://upload-images.jianshu.io/upload_images/4951489-fb4efa1c13c8bc88.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" data-original-src="http://upload-images.jianshu.io/upload_images/4951489-fb4efa1c13c8bc88.png?imageMogr2/auto-orient/strip" data-image-slug="fb4efa1c13c8bc88" data-width="1206" data-height="517"><br><div class="image-caption"></div>
</div><p>为了使用该方法，我们这么做：</p><p>上一个案例，我们得到：</p><p>val reduce = sc.textFile("/input/words.txt").flatMap(_.split(" ")).map((_, 1)).reduceByKey(_ + _)<br></p><p>然后：</p><p>val reverseRdd = reduce.map(x =&gt; (x._2, x._1))<br></p><p>然后我们看一眼这个RDD集合：</p><div class="image-package">
<img src="http://upload-images.jianshu.io/upload_images/4951489-156acd6a4674b8be.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" data-original-src="http://upload-images.jianshu.io/upload_images/4951489-156acd6a4674b8be.png?imageMogr2/auto-orient/strip" data-image-slug="156acd6a4674b8be" data-width="1529" data-height="49"><br><div class="image-caption"></div>
</div><p><b><b><b><b>Step2、直接使用sortByKey进行默认排序</b></b><br></b></b></p><p>val sortRdd = reverseRdd.sortByKey()<br></p><p><b><b><b><b><b><b>Step3、排序结束你不得给人家再反转回来？所以：</b></b><br></b></b></b></b></p><p>sortRdd.map(x =&gt; (x._2, x._1)).collect，如图：<br></p><div class="image-package">
<img src="http://upload-images.jianshu.io/upload_images/4951489-8fc3c3160537f2d1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" data-original-src="http://upload-images.jianshu.io/upload_images/4951489-8fc3c3160537f2d1.png?imageMogr2/auto-orient/strip" data-image-slug="8fc3c3160537f2d1" data-width="1525" data-height="44"><br><div class="image-caption"></div>
</div><p>当然了，以上分解步骤一气呵成最爽快：</p><p>sc.textFile("/input/words.txt")</p><p>.flatMap(_.split(" "))</p><p>.map((_, 1))</p><p>.reduceByKey(_ + _)</p><p>.map(x =&gt; (x._2, x._1))</p><p>.sortByKey()</p><p>.map(x =&gt; (x._2, x._1))</p><p>.collect<br></p><p><b><b><b><b><b><b><b><b>Step4、当然了，sortByKey方法也可以实现倒序，如：</b></b><br></b></b></b></b></b></b></p><p>sc.textFile("/input/words.txt")<br></p><p>.flatMap(_.split(" "))</p><p>.map((_, 1))</p><p>.reduceByKey(_ + _)</p><p>.map(x =&gt; (x._2, x._1))</p><p>.sortByKey(false)</p><p>.map(x =&gt; (x._2, x._1))</p><p>.collect</p><p><b><b><b><b><b><b><b><b><b><b>Step5、二次排序还可以使用top</b></b></b></b></b></b></b></b></b></b></p><p><b>top源码：</b></p><div class="image-package">
<img src="http://upload-images.jianshu.io/upload_images/4951489-702dbab88c8009ec.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" data-original-src="http://upload-images.jianshu.io/upload_images/4951489-702dbab88c8009ec.png?imageMogr2/auto-orient/strip" data-image-slug="702dbab88c8009ec" data-width="898" data-height="124"><br><div class="image-caption"></div>
</div><p>这是一个柯里化的函数，top命令是查看前多少条数据，如图可见，在查看之时，元素也是排序好的</p><p><b><b><b><b><b><b><b><b><b><b>比如：</b></b><br></b></b></b></b></b></b></b></b></p><p>sc.textFile("/input/words.txt")<br></p><p>.flatMap(_.split(" "))</p><p>.map((_, 1))</p><p>.reduceByKey(_ + _)</p><p>.map(x =&gt; (x._2, x._1))</p><p>.top(12)</p><p>输出如图：</p><div class="image-package">
<img src="http://upload-images.jianshu.io/upload_images/4951489-bab4b0afd3f0414e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" data-original-src="http://upload-images.jianshu.io/upload_images/4951489-bab4b0afd3f0414e.png?imageMogr2/auto-orient/strip" data-image-slug="bab4b0afd3f0414e" data-width="1599" data-height="90"><br><div class="image-caption"></div>
</div><h2>Spark运行模式之Standalone</h2><p><b>配置：spark-env.sh</b></p><p><b><br></b></p><div class="image-package">
<img src="http://upload-images.jianshu.io/upload_images/4951489-eaeffa3562767cd1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" data-original-src="http://upload-images.jianshu.io/upload_images/4951489-eaeffa3562767cd1.png?imageMogr2/auto-orient/strip" data-image-slug="eaeffa3562767cd1" data-width="429" data-height="260"><br><div class="image-caption"></div>
</div><p>Master节点：SPARK_MASTER_IP=z01<br></p><p>Master节点端口号：SPARK_MASTER_PORT=7077</p><p>Master WebUI端口号：SPARK_MASTER_WEBUI_PORT=8080</p><p>Worker节点可用CPU核心数：SPARK_WORKER_CORES=2</p><p>Worker可用内存：SPARK_WORKER_MEMORY=2g</p><p>Worker端口号：SPARK_WORKER_PORT=7078</p><p>Worker WebUI端口号：SPARK_WORKER_WEBUI_PORT=8081</p><p>允许在每台机器上开启几个Worker进程，默认为1个SPARK_WORKER_INSTANCES=1</p><p><b>配置：slaves</b></p><p>即配置允许哪几台机器当做Woker节点</p><div class="image-package">
<img src="http://upload-images.jianshu.io/upload_images/4951489-cb52984ec900ea31.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" data-original-src="http://upload-images.jianshu.io/upload_images/4951489-cb52984ec900ea31.png?imageMogr2/auto-orient/strip" data-image-slug="cb52984ec900ea31" data-width="911" data-height="124"><br><div class="image-caption"></div>
</div><p><b>以上配置完成后，scp到其他集群节点</b></p><p><b>启动：</b></p><p><b>Master</b></p><p><b>$ sbin/start-master.sh<br></b></p><p><b>Worker</b></p><p><b>$ sbin/start-slaves.sh<br></b></p><p><b>完成后通过z01:8080端口访问即可如图所示：</b></p><div class="image-package">
<img src="http://upload-images.jianshu.io/upload_images/4951489-bee19bfcf21098d9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" data-original-src="http://upload-images.jianshu.io/upload_images/4951489-bee19bfcf21098d9.png?imageMogr2/auto-orient/strip" data-image-slug="bee19bfcf21098d9" data-width="1920" data-height="810"><br><div class="image-caption"></div>
</div><p>也可以JPS看一下进程：</p><div class="image-package">
<img src="http://upload-images.jianshu.io/upload_images/4951489-a65da46df75fc8bd.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" data-original-src="http://upload-images.jianshu.io/upload_images/4951489-a65da46df75fc8bd.png?imageMogr2/auto-orient/strip" data-image-slug="a65da46df75fc8bd" data-width="640" data-height="360"><br><div class="image-caption"></div>
</div><p>在Standalone上运行Spark</p><p>首先，查看一下spark的帮助文档来引导该怎么做：</p><p>$ bin/spark-shell --help<br></p><p><br></p><div class="image-package">
<img src="http://upload-images.jianshu.io/upload_images/4951489-65a7567279acf574.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" data-original-src="http://upload-images.jianshu.io/upload_images/4951489-65a7567279acf574.png?imageMogr2/auto-orient/strip" data-image-slug="65a7567279acf574" data-width="1071" data-height="843"><br><div class="image-caption"></div>
</div><p>注意红框内的内容，那么接下来，我们应该知道怎么让spark运行在standalone上了：</p><p>$ bin/spark-shell --master spark://z01:7077<br></p><p>如图：注意红框内容</p><div class="image-package">
<img src="http://upload-images.jianshu.io/upload_images/4951489-88178969d2810a4d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" data-original-src="http://upload-images.jianshu.io/upload_images/4951489-88178969d2810a4d.png?imageMogr2/auto-orient/strip" data-image-slug="88178969d2810a4d" data-width="1915" data-height="646"><br><div class="image-caption"></div>
</div><p>尖叫提示：如果直接不加参数的使用spark-shell方式启动，则还是在本地模式（Local）启动的。</p><h2>Spark测试案例之Standalone模式<br>
</h2><p><b>案例一：跑一个一气呵成的WordCount</b></p><p>scala&gt; sc.textFile("/input/words.txt").flatMap(_.split(" ")).map((_, 1)).reduceByKey(_+_).collect<br></p><p>WEBUI，http://192.168.122.200:4040/jobs/ 如图：</p><div class="image-package">
<img src="http://upload-images.jianshu.io/upload_images/4951489-db1209e066285d0f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" data-original-src="http://upload-images.jianshu.io/upload_images/4951489-db1209e066285d0f.png?imageMogr2/auto-orient/strip" data-image-slug="db1209e066285d0f" data-width="1913" data-height="347"><br><div class="image-caption"></div>
</div><p>可以看到，有一个Job任务已经运行完毕了。</p><p><b>案例二：做一个每日的PV分析</b></p><p><b>Step1、首先，我们将网站的访问数据导入到hive当中，执行：</b></p><p>$ cat hql-file/track-log.hql<br></p><p>其中track-log.hql文件如下：</p><div class="image-package">
<img src="http://upload-images.jianshu.io/upload_images/4951489-c6496c0cf2744f54.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" data-original-src="http://upload-images.jianshu.io/upload_images/4951489-c6496c0cf2744f54.png?imageMogr2/auto-orient/strip" data-image-slug="c6496c0cf2744f54" data-width="1918" data-height="1494"><br><div class="image-caption">该部分内容可以参看Hive框架基础（一）</div>
</div><p><b>Step2、通过Hive查看track_log文件在哪</b></p><p>hive&gt; desc formatted track_log;<br></p><p>如图：注意红框内容，对于我们来讲，有用的即：/user/hive/warehouse/track_log/2015082818</p><div class="image-package">
<img src="http://upload-images.jianshu.io/upload_images/4951489-97f09fb6272c3fc0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" data-original-src="http://upload-images.jianshu.io/upload_images/4951489-97f09fb6272c3fc0.png?imageMogr2/auto-orient/strip" data-image-slug="97f09fb6272c3fc0" data-width="950" data-height="727"><br><div class="image-caption"></div>
</div><div class="image-package">
<img src="http://upload-images.jianshu.io/upload_images/4951489-c03182a3c0750a17.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" data-original-src="http://upload-images.jianshu.io/upload_images/4951489-c03182a3c0750a17.png?imageMogr2/auto-orient/strip" data-image-slug="c03182a3c0750a17" data-width="1243" data-height="261"><br><div class="image-caption"></div>
</div><p><b>Step3、将日志数据读入到RDD中等待分析</b></p><p>scala&gt; val rdd = sc.textFile("/user/hive/warehouse/track_log/2015082818")<br></p><p><b>Step4、清洗无效的数据，即空白行，以及url字段为空的，我们要过滤掉</b></p><p>1、先过滤空白行</p><p>2、再分割字段值</p><p>3、最后过滤url字段为空的</p><p>综合来写：</p><p>scala&gt; val validRdd = rdd.filter(line =&gt; line.length &gt; 0).map(_.split("\t")).filter(arr =&gt; arr(1).length &gt; 0)</p><p>当然了，此时你可以count一下，看看过滤后剩下多少数据<br></p><p>scala&gt; validRdd.count<br></p><div class="image-package">
<img src="http://upload-images.jianshu.io/upload_images/4951489-aed2f053c138860f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" data-original-src="http://upload-images.jianshu.io/upload_images/4951489-aed2f053c138860f.png?imageMogr2/auto-orient/strip" data-image-slug="aed2f053c138860f" data-width="1613" data-height="593"><br><div class="image-caption"></div>
</div><p><b>Step4、将URL做map映射，比如做出这样的映射：（今日日期, 1）</b></p><p><b>那么今日的日期在tracktime字段，属于分割后的数组的第17个索引处</b></p><p><b>在hive中我们查看一下该日期的格式：</b></p><p><b>hive&gt; select tracktime from track_log limit 1;<br></b></p><p><b>如图：</b></p><div class="image-package">
<img src="http://upload-images.jianshu.io/upload_images/4951489-f4abcedebaaea725.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" data-original-src="http://upload-images.jianshu.io/upload_images/4951489-f4abcedebaaea725.png?imageMogr2/auto-orient/strip" data-image-slug="f4abcedebaaea725" data-width="1258" data-height="436"><br><div class="image-caption"></div>
</div><p>那么截取出2015-08-28应该很容易，所以：</p><p>scala&gt; val mapRdd = validRdd.map(arr =&gt; (arr(17).substring(0, 10), 1))</p><p><b>Step5、你懂得，再来一个Reduce即可</b><br></p><p>scala&gt; val reduceRdd = mapRdd.reduceByKey(_ + _)</p><p>完事之后可以查看一下结果：</p><p>scala&gt; reduceRdd.collect<br></p><p>如图：</p><div class="image-package">
<img src="http://upload-images.jianshu.io/upload_images/4951489-374adecd0ea466a4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" data-original-src="http://upload-images.jianshu.io/upload_images/4951489-374adecd0ea466a4.png?imageMogr2/auto-orient/strip" data-image-slug="374adecd0ea466a4" data-width="1602" data-height="578"><br><div class="image-caption"></div>
</div><p>当然了也可以一气呵成走你：</p><p>scala&gt; sc.textFile("/user/hive/warehouse/track_log/2015082818")</p><p>.filter(line =&gt; line.length &gt; 0)</p><p>.map(_.split("\t"))</p><p>.filter(arr =&gt; arr(1).length &gt; 0)</p><p>.map(arr =&gt; (arr(17).substring(0, 10), 1))</p><p>.reduceByKey(_ + _)</p><p>.collect<br></p><p><b>Step6、我们使用Hive来验证一下</b></p><p><b>注意如果你的Yarn没有启动，需要将Hive设置成Local模式：</b></p><p>hive&gt; set hive.exec.mode.local.auto = true;</p><p>然后执行：</p><div class="image-package">
<img src="http://upload-images.jianshu.io/upload_images/4951489-da32d70f2489ee05.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" data-original-src="http://upload-images.jianshu.io/upload_images/4951489-da32d70f2489ee05.png?imageMogr2/auto-orient/strip" data-image-slug="da32d70f2489ee05" data-width="719" data-height="342"><br><div class="image-caption"></div>
</div><p>结果如图：</p><div class="image-package">
<img src="http://upload-images.jianshu.io/upload_images/4951489-2df41453e8a49b99.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" data-original-src="http://upload-images.jianshu.io/upload_images/4951489-2df41453e8a49b99.png?imageMogr2/auto-orient/strip" data-image-slug="2df41453e8a49b99" data-width="1329" data-height="816"><br><div class="image-caption"></div>
</div><p>对比可知，两个结果是一样的。</p><p><b>案例三：PV和UV分析</b></p><p><b>PV：即页面访问次数</b></p><p><b>UV：即不同用户访问页面次数</b></p><p><b>Step1、读取网站日志文件生成RDD对象</b></p><p>scala&gt; val rdd = sc.textFile("/user/hive/warehouse/track_log")<br></p><p><b>Step2、过滤不必要的数据，并生成map映射，注意此时的操作与之前的案例略有不同，请注意观察，如图：</b></p><p>scala&gt; val mapRdd = rdd.filter(_.length &gt; 0).map(line =&gt; {<br></p><p>| val arr = line.split("\t")</p><p>| val date = arr(17).substring(0, 10)</p><p>| val guid = arr(5)</p><p>| val url = arr(1)</p><p>| (date, guid, url)</p><p>| }).filter(tuple =&gt; tuple._3.length &gt; 0)</p><div class="image-package">
<img src="http://upload-images.jianshu.io/upload_images/4951489-d42191b4d978994a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" data-original-src="http://upload-images.jianshu.io/upload_images/4951489-d42191b4d978994a.png?imageMogr2/auto-orient/strip" data-image-slug="d42191b4d978994a" data-width="1248" data-height="250"><br><div class="image-caption"></div>
</div><p><b>Step3、可选步骤，此处可以将数据cache到内存中，注意，cache后，不会立刻缓存到内存中，需要执行一个action，比如count，take，collect都可以</b><br></p><p>scala&gt; mapRdd.cache<br></p><p>scala&gt; mapRdd.count<br></p><p><b>在此之后就可以在4040端口的页面是storge选项中看到缓存到内存中的数据信息，如图：</b></p><div class="image-package">
<img src="http://upload-images.jianshu.io/upload_images/4951489-0fcffcb6d995f4d8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" data-original-src="http://upload-images.jianshu.io/upload_images/4951489-0fcffcb6d995f4d8.png?imageMogr2/auto-orient/strip" data-image-slug="0fcffcb6d995f4d8" data-width="1902" data-height="272"><br><div class="image-caption"></div>
</div><p><b>Step4、统计PV</b><br></p><p>scala&gt; val pvRdd = mapRdd.map(tuple =&gt; (tuple._1, 1)).reduceByKey(_ + _)</p><p>scala&gt;  pvRdd.first，如图：</p><div class="image-package">
<img src="http://upload-images.jianshu.io/upload_images/4951489-3fdb66ba85aec03e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" data-original-src="http://upload-images.jianshu.io/upload_images/4951489-3fdb66ba85aec03e.png?imageMogr2/auto-orient/strip" data-image-slug="3fdb66ba85aec03e" data-width="480" data-height="25"><br><div class="image-caption"></div>
</div><p><b>Step5、UV统计</b><br></p><p>scala&gt; val uvRdd = mapRdd.map(tuple =&gt; (tuple._1 + "_" + tuple._2, 1)).distinct.map(tuple =&gt; {<br></p><p>val arr = tuple._1.split("_")</p><p>(arr(0), 1)</p><p>}).reduceByKey(_ + _)</p><div class="image-package">
<img src="http://upload-images.jianshu.io/upload_images/4951489-934fdeb767d351cd.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" data-original-src="http://upload-images.jianshu.io/upload_images/4951489-934fdeb767d351cd.png?imageMogr2/auto-orient/strip" data-image-slug="934fdeb767d351cd" data-width="1117" data-height="129"><br><div class="image-caption">此时可以自行使用uvRdd.first查看结果，不再展示</div>
</div><p><b>Step6、合并PV和UV的结果进行显示</b><br></p><p><b>union方式：</b></p><p>scala&gt; val pv_uvRdd = pvRdd.union(uvRdd)</p><p>scala&gt; pv_uvRdd.collect，如图：</p><div class="image-package">
<img src="http://upload-images.jianshu.io/upload_images/4951489-07668bd1f3067a2e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" data-original-src="http://upload-images.jianshu.io/upload_images/4951489-07668bd1f3067a2e.png?imageMogr2/auto-orient/strip" data-image-slug="07668bd1f3067a2e" data-width="854" data-height="34"><br><div class="image-caption"></div>
</div><p><b>join方式：</b></p><p>scala&gt; val pv_uvRdd = pvRdd.join(uvRdd)<br></p><p>scala&gt; pv_uvRdd.first，如图：<br></p><div class="image-package">
<img src="http://upload-images.jianshu.io/upload_images/4951489-75f096cf12a776cb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" data-original-src="http://upload-images.jianshu.io/upload_images/4951489-75f096cf12a776cb.png?imageMogr2/auto-orient/strip" data-image-slug="75f096cf12a776cb" data-width="668" data-height="37"><br><div class="image-caption"></div>
</div><p><b>验证：使用Hive或者SparkSQL验证结果一致性</b></p><p><b>首先创建SQL语句：</b></p><div class="image-package">
<img src="http://upload-images.jianshu.io/upload_images/4951489-8458587cd7eda370.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" data-original-src="http://upload-images.jianshu.io/upload_images/4951489-8458587cd7eda370.png?imageMogr2/auto-orient/strip" data-image-slug="8458587cd7eda370" data-width="547" data-height="504"><br><div class="image-caption"></div>
</div><p><b>SparkSQL方式：</b></p><p>scala&gt; val sql = """ 上边的SQL代码 """，如图：</p><div class="image-package">
<img src="http://upload-images.jianshu.io/upload_images/4951489-9732949a594a52d6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" data-original-src="http://upload-images.jianshu.io/upload_images/4951489-9732949a594a52d6.png?imageMogr2/auto-orient/strip" data-image-slug="9732949a594a52d6" data-width="609" data-height="836"><br><div class="image-caption"></div>
</div><p><b>然后执行：</b></p><p>scala&gt; val result = sqlContext.sql(sql)</p><div class="image-package">
<img src="http://upload-images.jianshu.io/upload_images/4951489-a776da853defab68.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" data-original-src="http://upload-images.jianshu.io/upload_images/4951489-a776da853defab68.png?imageMogr2/auto-orient/strip" data-image-slug="a776da853defab68" data-width="915" data-height="509"><br><div class="image-caption"></div>
</div><p>scala&gt; result.show()</p><div class="image-package">
<img src="http://upload-images.jianshu.io/upload_images/4951489-919e058d9705f6a5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" data-original-src="http://upload-images.jianshu.io/upload_images/4951489-919e058d9705f6a5.png?imageMogr2/auto-orient/strip" data-image-slug="919e058d9705f6a5" data-width="288" data-height="206"><br><div class="image-caption"></div>
</div><p><b>尖叫提示：如果你的hive使用了thrift的metastore方式，请把hive的hive-site.xml文件软连接到spark的conf目录下！！否则上述指令将会出现找不到table的错误。</b></p><p><b>HIVE方式：直接使用Hive客户端执行上面的SQL语句，如图：</b></p><div class="image-package">
<img src="http://upload-images.jianshu.io/upload_images/4951489-5495b647ef4422c4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" data-original-src="http://upload-images.jianshu.io/upload_images/4951489-5495b647ef4422c4.png?imageMogr2/auto-orient/strip" data-image-slug="5495b647ef4422c4" data-width="660" data-height="110"><br><div class="image-caption"></div>
</div><h2>Spark任务历史服务</h2><p>对于Yarn有mr-historyserver</p><p>对于Spark有SparkHistory</p><p>所以应该很容易明白这是一个任务日志的历史服务，比如你可以查看昨天半夜运行的任务情况。</p><p>开启这个服务也很简单：</p><p>可以参看：http://spark.apache.org/docs/1.6.1/monitoring.html</p><p><b>Step1、配置参数</b></p><p><b>配置：spark-env.sh，日志默认是保存在本地的，此刻我们将日志保存到HDFS系统当中如图：</b></p><div class="image-package">
<img src="http://upload-images.jianshu.io/upload_images/4951489-012482a19288be03.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" data-original-src="http://upload-images.jianshu.io/upload_images/4951489-012482a19288be03.png?imageMogr2/auto-orient/strip" data-image-slug="012482a19288be03" data-width="1169" data-height="291"><br><div class="image-caption"></div>
</div><p><b>配置：spark-defaults.conf，spark启动时默认加载的配置文件</b></p><div class="image-package">
<img src="http://upload-images.jianshu.io/upload_images/4951489-aef3e6fa3f8df567.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" data-original-src="http://upload-images.jianshu.io/upload_images/4951489-aef3e6fa3f8df567.png?imageMogr2/auto-orient/strip" data-image-slug="aef3e6fa3f8df567" data-width="1260" data-height="299"><br><div class="image-caption"></div>
</div><p><b>Step2、在HDFS系统中创建目录/user/z/spark-events</b></p><p><b>Step3、将配置文件重新scp到其他节点之后，重启服务，然后开启历史服务</b></p><p>$ sbin/start-history-server.sh</p><p>JPS看一眼：</p><div class="image-package">
<img src="http://upload-images.jianshu.io/upload_images/4951489-8da3b1369a8d97c6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" data-original-src="http://upload-images.jianshu.io/upload_images/4951489-8da3b1369a8d97c6.png?imageMogr2/auto-orient/strip" data-image-slug="8da3b1369a8d97c6" data-width="534" data-height="414"><br><div class="image-caption"></div>
</div><p>然后在浏览器打开：http://z01:18080/</p><p>如图：</p><div class="image-package">
<img src="http://upload-images.jianshu.io/upload_images/4951489-17f017f727c695d6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" data-original-src="http://upload-images.jianshu.io/upload_images/4951489-17f017f727c695d6.png?imageMogr2/auto-orient/strip" data-image-slug="17f017f727c695d6" data-width="1183" data-height="235"><br><div class="image-caption"></div>
</div><p><b>Step4、测试玩一玩？</b></p><p>$ bin/spark-shell --master spark://z01:7077<br></p><p>随便执行执行一个我们之前的案例任务，即可，运行几个任务，成功运行几个，再失败几个，如图：</p><div class="image-package">
<img src="http://upload-images.jianshu.io/upload_images/4951489-33d8830dcee24b72.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" data-original-src="http://upload-images.jianshu.io/upload_images/4951489-33d8830dcee24b72.png?imageMogr2/auto-orient/strip" data-image-slug="33d8830dcee24b72" data-width="1918" data-height="314"><br><div class="image-caption"></div>
</div><p>注意红框内容，如果你当前的spark-shell没有退出，那么该任务就是属于正在运行的任务。请自行切换观察即可。</p><h1>* 总结</h1><p><b>对于RDD到RDD的 操作，我们称之为Transformation操作</b></p><p>例如：我们在案例中使用的过滤，或者map，或者reduce等等</p><p><b>对RDD到其他类型的操作，我们称之为Action</b></p><p>例如：我们在案例中使用的top，或者take、collect等操作</p><p><b>另外RDD中的数据可以持久化到内存中来操作，使用：</b></p><p>rdd.cache来操作，比较适用于频繁使用的。</p><p>这一节我们大概了解了Spark的操作，也应该更加深刻的熟悉了Scala的操作。下一节我们针对Spark进行更深入的探讨。</p><hr><p>个人微博：http://weibo.com/seal13</p><p>QQ大数据技术交流群（广告勿入）：476966007</p><div class="image-package">
<img src="http://upload-images.jianshu.io/upload_images/4951489-ee111df51f51b83c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" data-original-src="http://upload-images.jianshu.io/upload_images/4951489-ee111df51f51b83c.png?imageMogr2/auto-orient/strip" data-image-slug="ee111df51f51b83c" data-width="280" data-height="355"><br><div class="image-caption"></div>
</div><hr><p>下一节：<a href="http://www.jianshu.com/p/ec5b8869f2f4" target="_blank">SparkCore基础（二）</a></p>
        </div>
      </div>
    </div>
  </body>
</html>
